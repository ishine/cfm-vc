{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(\"/workspace/pretrained_models/fcpe_c_v001.pt\", map_location=\"cpu\")\n",
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.cfm.dit import DiT\n",
    "import torch\n",
    "\n",
    "dit = DiT(\n",
    "    in_channels=128 * 2,\n",
    "    hidden_channels=192,\n",
    "    out_channels=128,\n",
    "    filter_channels=192 * 4,\n",
    "    dropout=0.05,\n",
    "    n_layers=8,\n",
    "    n_heads=4,\n",
    "    dim_head=64,\n",
    "    kernel_size=3,\n",
    "    utt_emb_dim=384,\n",
    "    use_skip_connections=False\n",
    ")\n",
    "\n",
    "# print dit parameter count\n",
    "print(f\"DiT parameter count: {sum(p.numel() for p in dit.parameters())}\")\n",
    "\n",
    "x = torch.randn(1, 128, 384)\n",
    "x_mask = torch.ones(1, 1, 384)\n",
    "mu = torch.randn(1, 128, 384)\n",
    "t = torch.Tensor([0.2])\n",
    "spks = torch.randn(1, 384)\n",
    "cond = torch.randn(1, 192, 32)\n",
    "cond_mask = torch.ones(1, 1, 32)\n",
    "\n",
    "dit(x, x_mask, mu, t, spks).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import SynthesizerTrn\n",
    "\n",
    "vc_model = SynthesizerTrn(\n",
    "    spec_channels=128,\n",
    "    hidden_channels=192,\n",
    "    filter_channels=768,\n",
    "    n_heads=2,\n",
    "    n_layers=6,\n",
    "    kernel_size=3,\n",
    "    p_dropout=0.1,\n",
    "    speaker_embedding=384,\n",
    "    n_speakers=10,\n",
    "    ssl_dim=768,\n",
    "    ppgs_dim=40,\n",
    ")\n",
    "\n",
    "c = torch.randn(1, 768, 565)\n",
    "c_lengths = torch.Tensor([565])\n",
    "ppgs = torch.randn(1, 40, 565)\n",
    "spec = torch.randn(1, 128, 565)\n",
    "f0 = torch.randn(1, 1, 565)\n",
    "uv = torch.ones(1, 565)\n",
    "g = torch.randn(1, 384)\n",
    "\n",
    "\n",
    "# print dit parameter count\n",
    "print(f\"VC model parameter count: {sum(p.numel() for p in vc_model.parameters())}\")\n",
    "\n",
    "# (prior_loss, diff_loss, f0_pred, lf0)\n",
    "vc_model(c=c, f0=f0, uv=uv, spec=spec, ppgs=ppgs, c_lengths=c_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = vc_model.infer(c=c, spec=spec, f0=f0, uv=uv, ppgs=ppgs, c_lengths=c_lengths)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, cond, cond_mask = vc_model.compute_conditional_latent([spec], [c_lengths])\n",
    "g.shape, cond.shape, cond_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.reference_encoder import MelStyleEncoder\n",
    "import torch\n",
    "\n",
    "\n",
    "mel_encoder = MelStyleEncoder(\n",
    "            in_channels=128,\n",
    "            hidden_channels=256,\n",
    "            utt_channels=512,\n",
    "            kernel_size=5,\n",
    "            p_dropout=0.1,\n",
    "            n_heads=4,\n",
    "            dim_head=64,\n",
    "        )\n",
    "\n",
    "# print dit parameter count\n",
    "print(f\"VC model parameter count: {sum(p.numel() for p in mel_encoder.parameters())}\")\n",
    "\n",
    "spec = torch.randn(1, 128, 56)\n",
    "spec_mask = torch.ones(1, 1, 56)\n",
    "\n",
    "\n",
    "g, cond, cond_mask = mel_encoder(spec, spec_mask)\n",
    "print(g, cond.shape, cond_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "train_all = [\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_borderlands2_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_baldursgate3_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_worldofwarcraft_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_mario_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_gametts_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/pl_archolos_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_borderlands2_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_warcraft_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_sqnarrator_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_emotional_train_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_emotional_train_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/ru_witcher3_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_witcher3_skyrim_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_fallout4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_naruto_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_kcd_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/pl_witcher3_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/de_diablo4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/en_diablo4_xphone.csv\",\n",
    "    # \"/workspace/metadata/filelists/xphoneBERT/fr_diablo4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/pl_diablo4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/ru_diablo4_xphone.csv\",\n",
    "    \"/workspace/metadata/filelists/xphoneBERT/ru_skyrim_xphone.csv\",\n",
    "    # \"/workspace/metadata/filelists/xphoneBERT/jp_one_piece_xphone.csv\",\n",
    "    # \"/workspace/metadata/filelists/xphoneBERT/jp_skyrim_xphone.csv\",\n",
    "    # \"/workspace/dataset/fr/Fallout4/fr_fallout4_xphone.csv\",\n",
    "    \"/workspace/dataset/de/Fallout4/de_fallout4_xphone.csv\",\n",
    "    \"/workspace/dataset/en/Fallout4/en_fallout4_xphone.csv\",\n",
    "]\n",
    "\n",
    "all_lines = []\n",
    "\n",
    "for file in train_all:\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        all_lines.extend(lines)\n",
    "\n",
    "random.shuffle(all_lines)\n",
    "\n",
    "files_max_per_speaker = 50\n",
    "min_audio_length = 0.3 * 22050\n",
    "max_audio_length = 12.0 * 22050\n",
    "\n",
    "speaker_files_dict = {}\n",
    "\n",
    "with open(\"/workspace/tts_train_slim.csv\", \"w\") as wf:\n",
    "    for line in all_lines:\n",
    "        cols = line.split(\"|\")\n",
    "        filename = cols[0]\n",
    "        speaker = cols[1]\n",
    "        language = cols[2]\n",
    "        text_orig = cols[3]\n",
    "\n",
    "        filename = filename.replace(\"/mnt/datasets/TTS_Data\", \"/workspace/dataset\")\n",
    "\n",
    "        if any(\n",
    "            v in text_orig\n",
    "            for v in [\"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \"v10\"]\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        if not Path(filename).exists():\n",
    "            continue\n",
    "\n",
    "        if max_audio_length < Path(filename).stat().st_size // 2 < min_audio_length:\n",
    "            continue\n",
    "\n",
    "        if any(char in \"#[]{}*\" for char in text_orig):\n",
    "            continue\n",
    "\n",
    "\n",
    "        if speaker not in speaker_files_dict:\n",
    "            speaker_files_dict[speaker] = []\n",
    "            speaker_files_dict[speaker].append(line)\n",
    "            wf.write(f\"{filename}|{speaker}|{language}|{text_orig}\")\n",
    "        else:\n",
    "            if len(speaker_files_dict[speaker]) < files_max_per_speaker:\n",
    "                speaker_files_dict[speaker].append(line)\n",
    "                wf.write(f\"{filename}|{speaker}|{language}|{text_orig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppgs\n",
    "\n",
    "# Load speech audio at correct sample rate\n",
    "audio = ppgs.load.audio(\"/workspace/dataset/ru/Witcher3/wavs/0x00100118.wav\")\n",
    "\n",
    "# Choose a gpu index to use for inference. Set to None to use cpu.\n",
    "gpu = None\n",
    "\n",
    "# Infer PPGs\n",
    "mu = ppgs.from_audio(audio, ppgs.SAMPLE_RATE, gpu=gpu)\n",
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.mel_processing import mel_spectrogram_torch\n",
    "import torchaudio\n",
    "import utils\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "random_ppg = random.choice(glob(\"/workspace/dataset/de/GameTTS/**/*.ppg_unit.pt\", recursive=True))\n",
    "feature = torch.load(random_ppg)\n",
    "\n",
    "audio, sr = torchaudio.load(\"/workspace/dataset/ru/Witcher3/wavs/0x00100118.wav\")\n",
    "mel = mel_spectrogram_torch(audio, 1024, 80, 22050, 256, 1024, 0, 8000)\n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import monotonic_align\n",
    "import torch\n",
    "\n",
    "ppg_embedding = torch.nn.Embedding(40, 80)\n",
    "\n",
    "y_mask = torch.ones(1, 1, mel.shape[-1])\n",
    "\n",
    "mu_x = ppg_embedding(mu).transpose(1, 2)\n",
    "print(mu_x.shape)\n",
    "x_mask= torch.ones(1, 1, mu.shape[-1])\n",
    "\n",
    "\n",
    "# attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
    "attn_mask = x_mask.unsqueeze(-1) * y_mask.unsqueeze(2)\n",
    "\n",
    "# Use MAS to find most likely alignment `attn` between ppg and mel-spectrogram\n",
    "with torch.no_grad():\n",
    "    const = -0.5 * math.log(2 * math.pi) * 80\n",
    "    factor = -0.5 * torch.ones(mu_x.shape, dtype=mu_x.dtype, device=mu_x.device)\n",
    "    y_square = torch.matmul(factor.transpose(1, 2), mel**2)\n",
    "    y_mu_double = torch.matmul(2.0 * (factor * mu_x).transpose(1, 2), mel)\n",
    "    mu_square = torch.sum(factor * (mu_x**2), 1).unsqueeze(-1)\n",
    "    log_prior = y_square - y_mu_double + mu_square + const\n",
    "\n",
    "    attn = (\n",
    "        monotonic_align.maximum_path(\n",
    "            log_prior,\n",
    "            attn_mask.squeeze(1),\n",
    "        )\n",
    "        .unsqueeze(1)\n",
    "        .detach()\n",
    "    )\n",
    "    \n",
    "print(torch.sum(attn, -1))\n",
    "logw_ = torch.log(1e-6 + torch.sum(attn, -1)) * x_mask\n",
    "logw_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def collate_1d_or_2d(values, pad_idx=0, left_pad=False, shift_right=False, max_len=None, shift_id=1):\n",
    "    if len(values[0].shape) == 1:\n",
    "        return collate_1d(values, pad_idx, left_pad, shift_right, max_len, shift_id)\n",
    "    else:\n",
    "        return collate_2d(values, pad_idx, left_pad, shift_right, max_len)\n",
    "\n",
    "\n",
    "def collate_1d(values, pad_idx=0, left_pad=False, shift_right=False, max_len=None, shift_id=1):\n",
    "    \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n",
    "    size = max(v.size(0) for v in values) if max_len is None else max_len\n",
    "    res = values[0].new(len(values), size).fill_(pad_idx)\n",
    "\n",
    "    def copy_tensor(src, dst):\n",
    "        assert dst.numel() == src.numel()\n",
    "        if shift_right:\n",
    "            dst[1:] = src[:-1]\n",
    "            dst[0] = shift_id\n",
    "        else:\n",
    "            dst.copy_(src)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n",
    "    return res\n",
    "\n",
    "\n",
    "def collate_2d(values, pad_idx=0, left_pad=False, shift_right=False, max_len=None):\n",
    "    \"\"\"Convert a list of 2d tensors into a padded 3d tensor.\"\"\"\n",
    "    size = max(v.size(0) for v in values) if max_len is None else max_len\n",
    "    res = values[0].new(len(values), size, values[0].shape[1]).fill_(pad_idx)\n",
    "\n",
    "    def copy_tensor(src, dst):\n",
    "        assert dst.numel() == src.numel()\n",
    "        if shift_right:\n",
    "            dst[1:] = src[:-1]\n",
    "        else:\n",
    "            dst.copy_(src)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n",
    "    return res\n",
    "\n",
    "\n",
    "def dedup_seq(seq):\n",
    "    B, L = seq.shape\n",
    "    vals, counts = [], []\n",
    "    for i in range(B):\n",
    "        val, count = zip(*[(k.item(), sum(1 for _ in g)) for k, g in groupby(seq[i])])\n",
    "        vals.append(torch.LongTensor(val))\n",
    "        counts.append(torch.LongTensor(count))\n",
    "    vals = collate_1d_or_2d(vals, 0)\n",
    "    counts = collate_1d_or_2d(counts, 0)\n",
    "    return vals, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "import ppgs\n",
    "import time\n",
    "from glob import glob\n",
    "import random\n",
    "from modules.mel_processing import mel_spectrogram_torch\n",
    "import torchaudio\n",
    "import utils\n",
    "\n",
    "\n",
    "random_wav = random.choice(glob(\"/workspace/dataset/de/GameTTS/**/*.ppg.pt\", recursive=True))\n",
    "\n",
    "audio, sr = torchaudio.load(random_wav.replace(\".ppg.pt\", \".wav\"))\n",
    "mel = mel_spectrogram_torch(audio, 1024, 80, 22050, 256, 1024, 0, 8000)\n",
    "print(mel.shape)\n",
    "\n",
    "np.random.seed(1234)\n",
    "kmeans = KMeans(n_clusters=40, verbose=False)\n",
    "feature = torch.load(random_wav)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "feature = feature.squeeze(0).numpy().T\n",
    "feature = feature.astype(np.float32)\n",
    "\n",
    "kmeans.fit(feature)\n",
    "features = torch.from_numpy(feature)\n",
    "features = kmeans.fit_predict(features)\n",
    "\n",
    "torch_features = torch.LongTensor(features).unsqueeze(0)\n",
    "torch_features, _ = dedup_seq(torch_features)\n",
    "print(torch_features.shape)\n",
    "print(f\"Time taken: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ppg = random.choice(glob(\"/workspace/dataset/de/GameTTS/**/*.ppg.pt\", recursive=True))\n",
    "feature = torch.load(random_ppg)\n",
    "\n",
    "feature = feature.squeeze(0).numpy().T\n",
    "feature = feature.astype(np.float32)\n",
    "feature.shape\n",
    "\n",
    "kmeans.fit(feature)\n",
    "features = torch.from_numpy(feature)\n",
    "features = kmeans.fit_predict(features)\n",
    "\n",
    "torch_features = torch.LongTensor(features).unsqueeze(0)\n",
    "torch_features, _ = dedup_seq(torch_features)\n",
    "torch_features = torch_features.squeeze(0)\n",
    "torch_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
